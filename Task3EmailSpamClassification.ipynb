{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh31RyOsiml9",
        "outputId": "e712b2df-89b4-43de-98c9-1303f48c29ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\")\n",
        "TEXT_COL = 'text' if 'text' in df.columns else max([c for c in df.columns if df[c].dtype=='object'], key=lambda c: df[c].astype(str).str.len().mean())\n",
        "texts = df[TEXT_COL].astype(str).fillna(\"\")\n",
        "\n",
        "def basic_clean(t):\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'http\\S+|www\\.\\S+', ' ', t)\n",
        "    t = re.sub(r'\\S+@\\S+', ' ', t)\n",
        "    t = re.sub(r'[^a-z\\s]', ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t\n",
        "\n",
        "clean_basic = texts.apply(basic_clean)\n",
        "\n",
        "tokens = clean_basic.apply(word_tokenize)\n",
        "\n",
        "sw = set(stopwords.words('english'))\n",
        "tokens_nostop = tokens.apply(lambda ws: [w for w in ws if w not in sw and len(w) > 1])\n",
        "\n",
        "ps = PorterStemmer()\n",
        "tokens_stem = tokens_nostop.apply(lambda ws: [ps.stem(w) for w in ws])\n",
        "clean_text_stem = tokens_stem.apply(lambda ws: \" \".join(ws))\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "tokens_lemma = tokens_nostop.apply(lambda ws: [wnl.lemmatize(w) for w in ws])\n",
        "clean_text_lemma = tokens_lemma.apply(lambda ws: \" \".join(ws))\n",
        "\n",
        "df['clean_text'] = clean_text_lemma\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdJitBJEQ-U",
        "outputId": "8d443df1-4d0b-4968-d3d3-eadf274041e0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = texts.apply(lambda t: re.sub(r'[^a-zA-Z\\s]', '', t.lower()))\n",
        "df['clean_text'] = texts.apply(lambda t: re.sub(r'[^a-zA-Z\\s]', '', t.lower()))\n",
        "\n",
        "df = df[df['clean_text'].str.strip().astype(bool)]\n"
      ],
      "metadata": {
        "id": "eC6DJMsBHrXv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Drop rows where clean_text is empty or only spaces\n",
        "df = df[df['clean_text'].str.strip().astype(bool)]\n",
        "\n",
        "# Fallback: if still empty, replace with placeholder\n",
        "df['clean_text'] = df['clean_text'].replace('', 'emptytext')\n",
        "\n",
        "cv = CountVectorizer(max_features=3000)\n",
        "X_bow = cv.fit_transform(df['clean_text'])\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "X_tfidf = tfidf.fit_transform(df['clean_text'])\n",
        "\n",
        "print(\"Bag of Words shape:\", X_bow.shape)\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni8ycpMVHddr",
        "outputId": "883ecb42-e513-4b99-99ca-0818cb9b1175"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words shape: (5569, 3000)\n",
            "TF-IDF shape: (5569, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sINyrrZ4INf-",
        "outputId": "969f83be-c374-496f-ee3f-6aa6dacf55dc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Category', 'Message', 'clean_text'], dtype='object')\n",
            "  Category                                            Message  \\\n",
            "0      ham  Go until jurong point, crazy.. Available only ...   \n",
            "1      ham                      Ok lar... Joking wif u oni...   \n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "3      ham  U dun say so early hor... U c already then say...   \n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
            "\n",
            "                                          clean_text  \n",
            "0  go until jurong point crazy available only in ...  \n",
            "1                            ok lar joking wif u oni  \n",
            "2  free entry in  a wkly comp to win fa cup final...  \n",
            "3        u dun say so early hor u c already then say  \n",
            "4  nah i dont think he goes to usf he lives aroun...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = X_tfidf\n",
        "y = df['Category']   # use Category as label\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb.predict(X_test)))\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr.predict(X_test)))\n",
        "\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, y_train)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H21A3NBuIDWN",
        "outputId": "10cc8d95-b815-4780-a585-0a03105c5fc8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.9748653500897666\n",
            "Logistic Regression Accuracy: 0.9721723518850988\n",
            "SVM Accuracy: 0.9874326750448833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evalution\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "models = {\n",
        "    \"Naive Bayes\": nb,\n",
        "    \"Logistic Regression\": lr,\n",
        "    \"SVM\": svm\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUD3D0E3JI5Z",
        "outputId": "37748c2d-e85a-4612-df5f-ceba53a87928"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Naive Bayes Results:\n",
            "Accuracy: 0.9748653500897666\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.99       955\n",
            "        spam       1.00      0.82      0.90       159\n",
            "\n",
            "    accuracy                           0.97      1114\n",
            "   macro avg       0.99      0.91      0.94      1114\n",
            "weighted avg       0.98      0.97      0.97      1114\n",
            "\n",
            "Confusion Matrix:\n",
            " [[955   0]\n",
            " [ 28 131]]\n",
            "\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.9721723518850988\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98       955\n",
            "        spam       0.98      0.82      0.89       159\n",
            "\n",
            "    accuracy                           0.97      1114\n",
            "   macro avg       0.98      0.91      0.94      1114\n",
            "weighted avg       0.97      0.97      0.97      1114\n",
            "\n",
            "Confusion Matrix:\n",
            " [[953   2]\n",
            " [ 29 130]]\n",
            "\n",
            "SVM Results:\n",
            "Accuracy: 0.9874326750448833\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      1.00      0.99       955\n",
            "        spam       0.98      0.93      0.95       159\n",
            "\n",
            "    accuracy                           0.99      1114\n",
            "   macro avg       0.98      0.96      0.97      1114\n",
            "weighted avg       0.99      0.99      0.99      1114\n",
            "\n",
            "Confusion Matrix:\n",
            " [[952   3]\n",
            " [ 11 148]]\n"
          ]
        }
      ]
    }
  ]
}